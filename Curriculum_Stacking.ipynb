{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc0dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "def updateModel(model, optimizer, freeze=False, mode=\"double\"):\n",
    "  \"\"\"\n",
    "    model     : a Pytorch Model\n",
    "    optimizer : a Pytorch optimizer\n",
    "    freeze    : Should the model freeze all of the previous encoder layers? Default = False\n",
    "    mode      : How should layers be added?\n",
    "                1. \"double\" (Default) --> Double the encoder layer\n",
    "                2. \"mean_double\"      --> Double the encoder layer, but each added layers are the mean of the previous layers\n",
    "                3. \"mean_single\"      --> Add a single layer, with it weights being the mean of the previous layers\n",
    "  \"\"\"\n",
    "\n",
    "  # Copy bert encoder layers\n",
    "  unfrozen = copy.deepcopy(model.bert.encoder.layer)\n",
    "\n",
    "  # What is the mode?\n",
    "  if (\"mean\" in mode.split('_')):\n",
    "    unfrozen = single_layer(unfrozen, \"mean\")\n",
    "    \n",
    "  # Freeze layers that have been trained\n",
    "  if (freeze):\n",
    "    for params in model.bert.encoder.layer.parameters():\n",
    "        params.requires_grad = False\n",
    "  \n",
    "\n",
    "  # Add layers of encoder to the model\n",
    "  if (mode == \"double\"):\n",
    "    for i in range(len(model.bert.encoder.layer)):\n",
    "      model.bert.encoder.layer.append(copy.deepcopy(unfrozen[i]))\n",
    "  elif (\"single\" in mode):\n",
    "    model.bert.encoder.layer.append(copy.deepcopy(unfrozen))\n",
    "  elif (\"double\" in mode):\n",
    "    for i in range(len(model.bert.encoder.layer)):\n",
    "      model.bert.encoder.layer.append(copy.deepcopy(unfrozen))\n",
    "  \n",
    "  new_optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "  return model, new_optimizer\n",
    "\n",
    "def single_layer(layer, mode):\n",
    "  sdUnfrozen = layer.state_dict()\n",
    "\n",
    "  sum_layer = OrderedDict()\n",
    "  done_queries = []\n",
    "  final_layer = OrderedDict()\n",
    "  mode = 'mean'\n",
    "  for key in sdUnfrozen:\n",
    "    q = '.'.join(key.split('.')[1:])\n",
    "    if (q not in done_queries):\n",
    "      done_queries.append(q)\n",
    "      lay_num = 0\n",
    "      for key in sdUnfrozen:\n",
    "        if ('.'.join(key.split('.')[1:]) == q):\n",
    "          if (lay_num == 0):\n",
    "            sum_layer[q] = copy.deepcopy(sdUnfrozen[key])\n",
    "          else:\n",
    "            sum_layer[q] += sdUnfrozen[key]\n",
    "          lay_num += 1\n",
    "      if (mode == 'mean'):\n",
    "        final_layer[q] = copy.deepcopy(sum_layer[q])/(lay_num)\n",
    "\n",
    "  singular_unfrozen = layer[0]\n",
    "  singular_unfrozen.load_state_dict(final_layer)\n",
    "\n",
    "  for params in singular_unfrozen.parameters():\n",
    "    params.requires_grad = True\n",
    "  \n",
    "  return singular_unfrozen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06182c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterias\n",
    "import math\n",
    "import string\n",
    "import syllables\n",
    "\n",
    "def criteria_length(texts: str) -> float:\n",
    "  \"\"\"\n",
    "  Length Criteria:\n",
    "    Number of Words in a corpus\n",
    "    Higher Value = Harder Corpus\n",
    "  \"\"\"\n",
    "  ret = {}\n",
    "  ret['dif'] = []\n",
    "  for text in texts: \n",
    "    ret['dif'].append(len(text.split()))\n",
    "  return ret\n",
    "###############################################################################\n",
    "def criteria_fre(texts: str, precision: int = 2) -> float:\n",
    "  \"\"\"\n",
    "  FRE Criteria:\n",
    "    Higher Value = Easier Corpus\n",
    "    Speed = Slow (Cause -> count_syllable)\n",
    "  \"\"\"\n",
    "  ret = {}\n",
    "  ret['dif'] = []\n",
    "  for text in texts: \n",
    "    sentence_count = len(text.split('.'))\n",
    "    if (text.endswith('.')):\n",
    "      sentence_count -= 1\n",
    "    word_count = len(text.split(' '))\n",
    "    syllable_count = syllables.estimate(text)\n",
    "    fre = 206.835 - 1.015*(word_count/sentence_count) - 84.6*(syllable_count/word_count)\n",
    "\n",
    "    ret['dif'].append(round(fre, precision))\n",
    "\n",
    "  return ret\n",
    "###############################################################################\n",
    "def criteria_fkg(texts: str, precision: int = 2) -> float:\n",
    "  \"\"\"\n",
    "  FKG Criteria:\n",
    "    Higher Value = Harder Corpus\n",
    "    Speed = Slow (Cause -> count_syllable)\n",
    "  \"\"\"\n",
    "  ret = {}\n",
    "  ret['dif'] = []\n",
    "  for text in texts: \n",
    "    sentence_count = len(text.split('.'))\n",
    "    if (text.endswith('.')):\n",
    "      sentence_count -= 1\n",
    "    word_count = len(text.split(' '))\n",
    "    syllable_count = count_syllable(text)\n",
    "    fkg = 0.39 * (word_count/sentence_count) + 11.8 * (syllable_count/word_count) - 15.59\n",
    "\n",
    "    ret['dif'].append(round(fkg, precision))\n",
    "\n",
    "  return ret \n",
    "###############################################################################\n",
    "def criteria_cli(texts: str, precision: int = 2) -> float:\n",
    "  \"\"\"\n",
    "  CLI Criteria:\n",
    "    Higher Value = Harder Corpus\n",
    "  \"\"\"\n",
    "  ret = {}\n",
    "  ret['dif'] = []\n",
    "  for text in texts: \n",
    "    sentence_count = len(text.split('.'))\n",
    "    if (text.endswith('.')):\n",
    "      sentence_count -= 1\n",
    "    word_count = len(text.split(' '))\n",
    "    letters = sum(c.isalpha() for c in text)\n",
    "    L = letters / word_count * 100\n",
    "    S = sentence_count / word_count * 100\n",
    "    cli = 0.0588*L - 0.296*S - 15.8\n",
    "\n",
    "    ret['dif'].append(round(cli, precision))\n",
    "  return ret\n",
    "###############################################################################\n",
    "def criteria_smog(texts: str, precision: int = 2) -> float:\n",
    "  \"\"\"\n",
    "  SMOG Criteria:\n",
    "    Higher Value = Harder Corpus\n",
    "    Corpus must have at least 30 sentences\n",
    "  \"\"\"\n",
    "\n",
    "  ret = {}\n",
    "  ret['dif'] = []\n",
    "  for text in texts: \n",
    "    sentence_count = len(text.split('.'))\n",
    "    if (text.endswith('.')):\n",
    "      sentence_count -= 1\n",
    "    \n",
    "    assert sentence_count >= 30, f\"Can only be used if the corpus contain at least 30 sentences\"\n",
    "    \n",
    "    polysyllables = count_syllable(text, ret='data')\n",
    "    polysyllables = [1 if e > 2 else 0 for e in polysyllables]\n",
    "    polysyllables = sum(polysyllables)\n",
    "\n",
    "    smog = 1.043 * math.sqrt(polysyllables * (30 / sentence_count)) + 3.1291\n",
    "\n",
    "    ret['dif'].append(round(smog, precision))\n",
    "\n",
    "  return ret\n",
    "\n",
    "###############################################################################\n",
    "def criteria_ttr(texts: str, precision: int = 2) -> float:\n",
    "  \"\"\"\n",
    "  TTR Criteria:\n",
    "    Higher Value = Harder Corpus\n",
    "    Range = [0, 1]\n",
    "  \"\"\"\n",
    "  ret = {}\n",
    "  ret['dif'] = []\n",
    "  for text in texts:  \n",
    "    words = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = words.lower()\n",
    "    words = words.split(' ')\n",
    "    unique_words = set(words)\n",
    "    ret['dif'].append(round(len(unique_words)/len(words), precision)) \n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74bdd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model_name, output_dir, data_path,\n",
    "          checkpoint, progressive_stacking, curriculum_learning,\n",
    "          curriculum, curriculum_criteria,\n",
    "          train_batch_size, num_epoch, accum_iter,\n",
    "          update_steps, stacking_strategy):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        model_name : string = What to save the model as\n",
    "        output_dir : string = Folder location to save the model\n",
    "        data_path  : string = File location to the dataset that should be used\n",
    "        checkpoint : string = Location to a folder with a pytorch_model.bin and config.json OR\n",
    "                              String with hugging-face checkpoint\n",
    "        progressive_stacking : bool = Do you want to use progressive_stacking?\n",
    "        curriculum_learning  : bool = Do you want to use curriculum_learning?\n",
    "        curriculum : string = ['easy_first', 'hard_first'] (Choose one), used if curriculum_learning = True\n",
    "        curriculum_criteria : function = Look above for list of criteria, choose one. \n",
    "        train_batch_size : int = Size of batch_size\n",
    "        accum_iter       : int = Defaults to 1. Allows for gradient accumulation.\n",
    "        update_steps     : list of float = List of percentage, tells the model when to update\n",
    "                                           Used if progressive_stacking = True\n",
    "                                           Example = [0.1, 0.25]\n",
    "        stacking_strategy : string = One of:\n",
    "                                     \"double\"   --> Duplicate and stack\n",
    "                                     \"mean_double\" --> Find mean of all previous layer and stack\n",
    "                                     \"mean_single\" --> Fine mean of all previous layer, add single layer\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    # Encoding Helpers\n",
    "    def encode_with_truncation(examples, max_length = 512):\n",
    "        \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "        return tokenizer(examples[\"text\"], truncation=True, return_special_tokens_mask=True)\n",
    "\n",
    "    def encode_without_truncation(examples):\n",
    "        \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "        return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "    \n",
    "    \n",
    "    def curriculumLearning_pipeline(data, criteria_func, curriculum = 'easy_first', verbose = False):\n",
    "        assert curriculum in ['easy_first', 'hard_first'], f\"curriculum must be 'easy_first' or 'hard_first'\"\n",
    "\n",
    "        if (verbose):\n",
    "            print(f'Automatically rating corpus difficulty...')\n",
    "        data = data.map(lambda x : criteria_func(x['text']), batched=True)\n",
    "\n",
    "        if (verbose):\n",
    "            print(f'Sorting corpus based on difficulty with rule of ', end ='')\n",
    "            if (curriculum == 'easy_first'):\n",
    "                print('easy to hard')\n",
    "            else:\n",
    "                print('hard to easy')\n",
    "\n",
    "        if (curriculum == 'easy_first' and criteria_func.__name__ == 'criteria_fre'):\n",
    "            data = data.sort('dif', reverse=True)\n",
    "        elif (curriculum == 'hard_first' and criteria_func.__name__ == 'criteria_fre'):\n",
    "            data = data.sort('dif')\n",
    "        elif (curriculum == 'easy_first'):\n",
    "            data = data.sort('dif')\n",
    "        elif (curriculum == 'hard_first'):\n",
    "            data = data.sort('dif', reverse=True)\n",
    "\n",
    "        data = data.remove_columns(['dif'])\n",
    "        return data\n",
    "    \n",
    "    # Load dataset\n",
    "    text_dataset = load_dataset(\"text\", data_files=data_path)['train']\n",
    "    print(f\"Text dataset: {text_dataset}\")\n",
    "    \n",
    "    # Get Models and Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model_conf = BertForMaskedLM.from_pretrained(checkpoint).config\n",
    "    model = BertForMaskedLM(model_conf)\n",
    "    \n",
    "    if (progressive_stacking):\n",
    "        # Set Model to 3 Encoder Layer\n",
    "        for i in range(11, 2, -1):\n",
    "          del model.bert.encoder.layer[i]\n",
    "    \n",
    "    if (curriculum_learning):\n",
    "        text_dataset = curriculumLearning_pipeline(text_dataset, curriculum_criteria, curriculum = curriculum, verbose=True)\n",
    "        \n",
    "    # Tokenize Dataset\n",
    "    tokenized_dataset = text_dataset.map(encode_with_truncation, batched=True)\n",
    "    \n",
    "    # Set Devices\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Set Up Data Collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    )\n",
    "    \n",
    "    # Set up DataLoader\n",
    "    if (curriculum_learning):\n",
    "        shuffle = False\n",
    "    else:\n",
    "        shuffle = True\n",
    "    train_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = train_batch_size, collate_fn = data_collator, shuffle=True)\n",
    "    print(f\"Len of train_dataloader = {len(train_dataloader)}\")\n",
    "    \n",
    "    # Preparing Tensors\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Set Up Constants (Insert scheduler here)\n",
    "    num_training_steps = int(num_epoch / accum_iter * len(train_dataloader))\n",
    "    \n",
    "    # Update steps\n",
    "    if (progressive_stacking):\n",
    "        update_steps = [int(e * num_training_steps) for e in update_steps]\n",
    "        print(f\"Model will be updated at {update_steps}\")\n",
    "    else:\n",
    "        update_steps = [-1]\n",
    "        \n",
    "    # Main Training Loop\n",
    "    print(\"Training: \")\n",
    "    print(f\"Train Total Batch Size = {accum_iter * train_batch_size}\")\n",
    "    print(f\"Number of steps to be taken = {num_training_steps}\")\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    model.train()\n",
    "    step = 0\n",
    "    start_time = time.time()\n",
    "    sum_elapsed_time = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        for (i, batch) in enumerate(train_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Accumulation\n",
    "            if (((i % accum_iter) == 0) or (i + 1 == len(train_dataloader))):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                wandb.log({\"loss\": loss})\n",
    "                elapsed_time = time.time() - start_time\n",
    "                sum_elapsed_time += elapsed_time\n",
    "                wandb.log({\"time(s)\" : elapsed_time})\n",
    "                wandb.log({\"total_time(s)\" : sum_elapsed_time})\n",
    "                step += 1\n",
    "\n",
    "                if (step in update_steps):\n",
    "                    print(\"Updating Model:\")\n",
    "                    print(f\"Previously at {len(model.bert.encoder.layer)}\")\n",
    "                    model, optimizer = updateModel(model, optimizer, freeze=False, mode=stacking_strategy)\n",
    "                    print(f\"Currently at {len(model.bert.encoder.layer)}\")\n",
    "\n",
    "                start_time = time.time()\n",
    "    \n",
    "    SAVE_PATH = f\"model/{model_name}\"\n",
    "    model.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0329a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b215a6c16c929d49\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-b215a6c16c929d49/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2f95226f6b46d39934eb4250982169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text dataset: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10306022\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-b215a6c16c929d49/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-36800f01a61b2d90.arrow\n",
      "Loading cached sorted indices for dataset at /root/.cache/huggingface/datasets/text/default-b215a6c16c929d49/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-9686ff240d16a05f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically rating corpus difficulty...\n",
      "Sorting corpus based on difficulty with rule of easy to hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-b215a6c16c929d49/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-e14c27a178f2a397.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train_dataloader = 322064\n",
      "Model will be updated at [2012, 4025, 6038, 8051, 10064, 12077, 14090, 16103, 18116]\n",
      "Training: \n",
      "Train Total Batch Size = 256\n",
      "Number of steps to be taken = 40258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dd3d70800f4cfb8b268007041334be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Model:\n",
      "Previously at 3\n",
      "Currently at 4\n",
      "Updating Model:\n",
      "Previously at 4\n",
      "Currently at 5\n",
      "Updating Model:\n",
      "Previously at 5\n",
      "Currently at 6\n",
      "Updating Model:\n",
      "Previously at 6\n",
      "Currently at 7\n",
      "Updating Model:\n",
      "Previously at 7\n",
      "Currently at 8\n",
      "Updating Model:\n",
      "Previously at 8\n",
      "Currently at 9\n",
      "Updating Model:\n",
      "Previously at 9\n",
      "Currently at 10\n",
      "Updating Model:\n",
      "Previously at 10\n",
      "Currently at 11\n",
      "Updating Model:\n",
      "Previously at 11\n",
      "Currently at 12\n"
     ]
    }
   ],
   "source": [
    "single_update_steps = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "double_update_steps = [0.1, 0.25]\n",
    "\n",
    "train(model_name = \"5p_mix_easy-first_single-mean\",\n",
    "      output_dir = \"model\",\n",
    "      data_path = \"data/en-5.txt\",\n",
    "      checkpoint = \"bert-base-uncased\", \n",
    "      progressive_stacking = True, \n",
    "      curriculum_learning = True,\n",
    "      curriculum = \"easy_first\", \n",
    "      curriculum_criteria = criteria_ttr,\n",
    "      train_batch_size = 32, \n",
    "      num_epoch = 1, \n",
    "      accum_iter = 8,\n",
    "      update_steps = single_update_steps, \n",
    "      stacking_strategy = \"mean_single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ff5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcfb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
