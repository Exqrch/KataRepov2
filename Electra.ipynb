{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3827bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraConfig, ElectraForPreTraining, AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "# Set up Generator + Discriminator [First]\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "generator = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "discriminator = ElectraForPreTraining.from_pretrained('google/electra-base-discriminator')\n",
    "discriminator_config = discriminator.config\n",
    "discriminator = ElectraForPreTraining(discriminator_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4622dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraConfig, ElectraForPreTraining, AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "# Set up Generator + Discriminator [Continue Training]\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "generator = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "discriminator = ElectraForPreTraining.from_pretrained('model/electra-5p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea5795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b215a6c16c929d49\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-b215a6c16c929d49/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f68ea82b254060b66f529a230d63c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-b215a6c16c929d49/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-4fe48c5930fdda04.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a152e7983d4c40a798a853cf52200d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train:\n",
      "batch size      = 32\n",
      "gradient accum  = 8\n",
      "dataloader size = 322064\n",
      "\n",
      "loss at 100 = 0.16511881351470947\n",
      "loss at 200 = 0.15457402169704437\n",
      "loss at 300 = 0.16783452033996582\n",
      "loss at 400 = 0.14803257584571838\n",
      "loss at 500 = 0.1818133145570755\n",
      "loss at 600 = 0.17545990645885468\n",
      "loss at 700 = 0.18486371636390686\n",
      "loss at 800 = 0.16348466277122498\n",
      "loss at 900 = 0.17506049573421478\n",
      "loss at 1000 = 0.1657273769378662\n",
      "loss at 1100 = 0.18806779384613037\n",
      "loss at 1200 = 0.15661698579788208\n"
     ]
    }
   ],
   "source": [
    "from datasets import (load_dataset)\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import html\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "def mask_input(input_ids, attention_mask):\n",
    "  \n",
    "  masked = input_ids.clone()\n",
    "\n",
    "  for i in range(input_ids.shape[0]):\n",
    "    rand = torch.rand(input_ids[i].shape).to(input_ids.device)\n",
    "    mask_arr = (rand < 0.15) * (input_ids[i] != 101) * (input_ids[i] != 102) * (attention_mask[i] == 1)\n",
    "    #print(mask_arr)\n",
    "    selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "    #print(\"selection is\", selection)\n",
    "    masked[i, selection] = 103\n",
    "    #print(\"====\")\n",
    "\n",
    "  return masked\n",
    "\n",
    "# Params:\n",
    "accum_iter = 8\n",
    "batch_size = 32\n",
    "\n",
    "# Set up Device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Set up dataset\n",
    "text_dataset = load_dataset(\"text\", data_files=\"data/en-5.txt\")['train']\n",
    "tokenized_dataset = text_dataset.map(\n",
    "    lambda examples: tokenizer(examples[\"text\"], truncation=True, return_special_tokens_mask=True), \n",
    "    batched = True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=True\n",
    ")\n",
    "\n",
    "# Train DataLoader\n",
    "train_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, collate_fn = data_collator, shuffle=True)\n",
    "\n",
    "# Move Model To Device\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Generator only used for inferrence, so, freezed\n",
    "for params in generator.parameters():\n",
    "  params.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(discriminator.parameters(), lr=5e-5)\n",
    "\n",
    "# Set Up Progress bar + Gradient Accumulation, ignore \"loss_step\"\n",
    "if accum_iter == None or accum_iter < 1:\n",
    "    accum_iter = 1\n",
    "num_training_steps = int(1 / accum_iter * len(train_dataloader))\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "discriminator.train()\n",
    "loss_step = 0\n",
    "print(f\"\"\"Starting to train:\n",
    "batch size      = {batch_size}\n",
    "gradient accum  = {accum_iter}\n",
    "dataloader size = {len(train_dataloader)}\n",
    "\"\"\")\n",
    "for epoch in range(1):\n",
    "  for (i, batch) in enumerate(train_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # Set a new batch (batch for discriminator)\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    token_type_ids = batch[\"token_type_ids\"]\n",
    "    labels = input_ids.clone()\n",
    "    labels = torch.where(attention_mask == 1, labels, -100)\n",
    "\n",
    "    masked_input = mask_input(input_ids, attention_mask)\n",
    "\n",
    "    generator_output = generator(input_ids = masked_input, \n",
    "                                 attention_mask = attention_mask, \n",
    "                                 token_type_ids = token_type_ids) #Labels not needed, freezing it\n",
    "\n",
    "    softmaxValue = torch.nn.functional.softmax(generator_output.logits, dim = 1)\n",
    "    optToken = torch.argmax(softmaxValue, dim = 2)\n",
    "    new_inputs = torch.where((masked_input == 103), optToken, input_ids)\n",
    "\n",
    "    if labels is not None:\n",
    "      labels = torch.where((labels != -100), (new_inputs != labels).type_as(labels), labels)\n",
    "\n",
    "    discriminator_output = discriminator(input_ids = new_inputs, \n",
    "                                         attention_mask = attention_mask, \n",
    "                                         token_type_ids = token_type_ids, \n",
    "                                         labels=labels)\n",
    "    loss = discriminator_output.loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    if (((i % accum_iter) == 0) or (i + 1 == len(train_dataloader)) and (i != 0)):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        loss_step += 1\n",
    "        \n",
    "        if (loss_step % 100 == 0 and loss_step != 0):\n",
    "            print(f\"loss at {loss_step} = {loss}\")\n",
    "        \n",
    "discriminator.save_pretrained(\"model/electra-5p-epoch_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b94a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
