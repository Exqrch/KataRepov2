{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3827bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraConfig, ElectraForPreTraining, AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "generator = BertForMaskedLM.from_pretrained('prajjwal1/bert-small')\n",
    "discriminator = ElectraForPreTraining.from_pretrained('google/electra-base-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fea5795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a8c60a5cc6356686\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-a8c60a5cc6356686/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74a64a6a60e44d7970b0822621dec2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-a8c60a5cc6356686/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-bdaec421163bbd5c.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08ca6e208934fb98d110084cbda38d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 629 = 0.12290767580270767\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21407/1191557316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mreal_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_tokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m103\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m           \u001b[0mreal_tokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mreal_sentence_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import (load_dataset)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import html\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "text_dataset = load_dataset(\"text\", data_files=\"data/en-1.txt\")\n",
    "tokenized_dataset = text_dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True, return_special_tokens_mask=True), batched = True)\n",
    "tokenized_dataset = tokenized_dataset['train']\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, collate_fn = data_collator, shuffle=True)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Generator only used for inferrence\n",
    "for params in generator.parameters():\n",
    "  params.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(discriminator.parameters(), lr=5e-5)\n",
    "\n",
    "num_training_steps = int(1 / 8 * len(train_dataloader))\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "accum_iter = 8\n",
    "discriminator.train()\n",
    "loss_step = 0\n",
    "for epoch in range(1):\n",
    "  for (i, batch) in enumerate(train_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # Get Discriminator Labels (input_ids)\n",
    "    real_sentence_batch = {}\n",
    "    real_sentence_batch['input_ids'] = []\n",
    "    real_sentence_batch['attention_mask'] = copy.deepcopy(batch['attention_mask'])\n",
    "    real_sentence_batch['token_type_ids'] = copy.deepcopy(batch['token_type_ids'])\n",
    "    for m in range(len(batch['input_ids'])):\n",
    "      real_tokenized = copy.deepcopy(batch['input_ids'][m])\n",
    "      for n in range(len(real_tokenized)):\n",
    "        if (real_tokenized[n] == 103):\n",
    "          real_tokenized[n] = batch['labels'][m][n]\n",
    "      real_sentence_batch['input_ids'].append(real_tokenized)\n",
    "    real_sentence_batch['input_ids'] = torch.stack(real_sentence_batch['input_ids'])\n",
    "\n",
    "    logits = generator(**batch).logits\n",
    "    mask_token_index = (batch['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "    entry, index = mask_token_index[0].tolist(), mask_token_index[1].tolist()\n",
    "    for (x, y) in zip(entry, index):\n",
    "      batch['input_ids'][x][y] = logits[x][y].argmax(axis=-1)\n",
    "\n",
    "    real_sentence_batch['labels'] = []\n",
    "    for x in range(len(batch['input_ids'])):\n",
    "      label = [0] * len(batch['input_ids'][x])\n",
    "      for y in range(len(label)):\n",
    "        if (batch['input_ids'][x][y] != 0):\n",
    "          label[y] = 1 if batch['input_ids'][x][y] != real_sentence_batch['input_ids'][x][y] else 0\n",
    "      real_sentence_batch['labels'].append(label)\n",
    "    real_sentence_batch['labels'] = torch.tensor(real_sentence_batch['labels'])\n",
    "    real_sentence_batch = {k: v.to(device) for k, v in real_sentence_batch.items()}\n",
    "    discriminator_output = discriminator(input_ids = batch['input_ids'].to(device), \n",
    "                                         attention_mask = batch['attention_mask'].to(device),\n",
    "                                         token_type_ids = batch['token_type_ids'].to(device), \n",
    "                                         labels = real_sentence_batch['labels'].to(device))\n",
    "    loss = discriminator_output.loss\n",
    "    loss.backward()\n",
    "    print(f\"loss {i} = {loss}\", end='\\r')\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    if (((i % accum_iter) == 0) or (i + 1 == len(train_dataloader)) and (i != 0)):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        loss_step += 1\n",
    "        \n",
    "discriminator.save_pretrained(\"model/electra-1p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f1d7934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens      :  ['[CLS]', 'i', 'like', 'pie', '.', '[SEP]']\n",
      "Predictions :  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0] Predictions:  tensor([[1., 1., 1., 1., 1., 1.]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Labels      :  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "real_sentence = \"I like pie.\"\n",
    "corrupted_sentence = \"I [MASK] pie.\"\n",
    "tokenized_corrupted_sentence = tokenizer(corrupted_sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = generator(**tokenized_corrupted_sentence.to(device)).logits\n",
    "\n",
    "mask_token_index = (tokenized_corrupted_sentence.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "corrupted_sentence = corrupted_sentence.replace('[MASK]', tokenizer.decode(predicted_token_id))\n",
    "# corrupted_sentence = \"The capital of Indonesia is USA.\"\n",
    "tokenized_real_sentence =  tokenizer(real_sentence, return_tensors=\"pt\")\n",
    "tokenized_corrupted_sentence = tokenizer(corrupted_sentence, return_tensors=\"pt\")\n",
    "\n",
    "discriminator_outputs = discriminator(tokenized_corrupted_sentence.input_ids.to(device))\n",
    "predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n",
    "print(\"Tokens      : \", tokenizer.tokenize(corrupted_sentence, add_special_tokens=True))\n",
    "print(\"Predictions : \", predictions.squeeze().tolist(), \"Predictions: \", (torch.sign(discriminator_outputs[0]) + 1) / 2)\n",
    "print(\"Labels      : \", [float(0) if(i == j) else float(1) for (i,j) in zip(tokenized_real_sentence.input_ids.squeeze().tolist(), tokenized_corrupted_sentence.input_ids.squeeze().tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980bb4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
